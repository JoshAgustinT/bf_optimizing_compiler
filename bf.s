.file	"bf compiler"
.section .data
.p2align 5
.four_offset_mask:
.quad   282578783371521
.quad   282578783371521
.quad   282578783371521
.quad  282578783371521
.p2align 5
.one_offset_mask:
.quad   0
.quad   0
.quad   0
.quad   0
.p2align 5
.two_offset_mask:
.quad   281479271743489
.quad   281479271743489
.quad   281479271743489
.quad  281479271743489
.p2align 5
.eight_offset_mask:
.quad   282578800148737
.quad   282578800148737
.quad   282578800148737
.quad   282578800148737
.p2align 5
.sixteen_offset_mask:
.quad   72340172838076673
.quad   282578800148737
.quad   72340172838076673
.quad  282578800148737
.p2align 5
.thirty_two_offset_mask:
.quad   72340172838076673
.quad   72340172838076673
.quad   72340172838076673
.quad  282578800148737
.text
.section	.text
.globl	main
.type	main, @function

main:
pushq	%rbp
movq	%rsp, %rbp
subq	$16, %rsp
movl	$1048576, %edi
call	malloc@PLT
movq	%rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $524288, %rax
movq    %rax, -8(%rbp)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_0
start_loop_0:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_1
start_loop_1:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_1
end_loop_1:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_0
end_loop_0:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_2
start_loop_2:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_3
start_loop_3:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_4
start_loop_4:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_4
end_loop_4:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_3
end_loop_3:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_2
end_loop_2:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_5
start_loop_5:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_5
end_loop_5:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_6
start_loop_6:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_6
end_loop_6:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_7
start_loop_7:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_8
start_loop_8:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_8
end_loop_8:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_9
start_loop_9:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_10
start_loop_10:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_10
end_loop_10:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_11
start_loop_11:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_12
start_loop_12:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_12
end_loop_12:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_11
end_loop_11:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_13
start_loop_13:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_14
start_loop_14:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_15
start_loop_15:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_15
end_loop_15:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_14
end_loop_14:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_13
end_loop_13:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_16
start_loop_16:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_16
end_loop_16:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_17
start_loop_17:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_17
end_loop_17:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_18
start_loop_18:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_18
end_loop_18:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_19
start_loop_19:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_20
start_loop_20:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_20
end_loop_20:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_19
end_loop_19:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_21
start_loop_21:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_22
start_loop_22:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_23
start_loop_23:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_23
end_loop_23:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_22
end_loop_22:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_21
end_loop_21:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_24
start_loop_24:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_24
end_loop_24:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_25
start_loop_25:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_25
end_loop_25:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_26
start_loop_26:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_26
end_loop_26:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_27
start_loop_27:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_27
end_loop_27:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-4, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-4, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_28
start_loop_28:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_29
start_loop_29:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_29
end_loop_29:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_28
end_loop_28:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_30
start_loop_30:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_31
start_loop_31:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_32
start_loop_32:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_32
end_loop_32:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_31
end_loop_31:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_30
end_loop_30:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_33
start_loop_33:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_33
end_loop_33:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_34
start_loop_34:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_34
end_loop_34:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_35
start_loop_35:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_35
end_loop_35:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_36
start_loop_36:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_37
start_loop_37:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_38
start_loop_38:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_38
end_loop_38:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_37
end_loop_37:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_39
start_loop_39:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_40
start_loop_40:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_41
start_loop_41:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_41
end_loop_41:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_40
end_loop_40:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_39
end_loop_39:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_36
end_loop_36:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_42
start_loop_42:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_42
end_loop_42:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_43
start_loop_43:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_44
start_loop_44:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_45
start_loop_45:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_45
end_loop_45:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_44
end_loop_44:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_46
start_loop_46:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_47
start_loop_47:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_48
start_loop_48:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_48
end_loop_48:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_47
end_loop_47:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_46
end_loop_46:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_49
start_loop_49:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_49
end_loop_49:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_50
start_loop_50:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_50
end_loop_50:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_43
end_loop_43:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_51
start_loop_51:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_52
start_loop_52:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_53
start_loop_53:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_53
end_loop_53:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_52
end_loop_52:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_54
start_loop_54:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_55
start_loop_55:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_56
start_loop_56:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_56
end_loop_56:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_55
end_loop_55:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_54
end_loop_54:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_51
end_loop_51:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_57
start_loop_57:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_57
end_loop_57:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_58
start_loop_58:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_59
start_loop_59:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_60
start_loop_60:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_60
end_loop_60:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_59
end_loop_59:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_61
start_loop_61:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_62
start_loop_62:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_63
start_loop_63:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_63
end_loop_63:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_62
end_loop_62:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_61
end_loop_61:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_64
start_loop_64:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_64
end_loop_64:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_65
start_loop_65:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_65
end_loop_65:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_58
end_loop_58:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_66
start_loop_66:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_67
start_loop_67:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_68
start_loop_68:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_68
end_loop_68:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_67
end_loop_67:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_69
start_loop_69:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_70
start_loop_70:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_71
start_loop_71:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_71
end_loop_71:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_70
end_loop_70:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_69
end_loop_69:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_66
end_loop_66:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_72
start_loop_72:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_72
end_loop_72:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_73
start_loop_73:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_74
start_loop_74:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_75
start_loop_75:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_75
end_loop_75:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_74
end_loop_74:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_76
start_loop_76:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_77
start_loop_77:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_78
start_loop_78:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_78
end_loop_78:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_77
end_loop_77:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_76
end_loop_76:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_79
start_loop_79:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_79
end_loop_79:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_80
start_loop_80:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_80
end_loop_80:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_73
end_loop_73:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_81
start_loop_81:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_81
end_loop_81:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_82
start_loop_82:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_83
start_loop_83:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_83
end_loop_83:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_84
start_loop_84:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_84
end_loop_84:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_82
end_loop_82:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_85
start_loop_85:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_86
start_loop_86:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_87
start_loop_87:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_87
end_loop_87:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_86
end_loop_86:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_88
start_loop_88:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_89
start_loop_89:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_90
start_loop_90:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_90
end_loop_90:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_89
end_loop_89:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_88
end_loop_88:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_85
end_loop_85:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_91
start_loop_91:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_91
end_loop_91:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_92
start_loop_92:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_93
start_loop_93:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_94
start_loop_94:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_94
end_loop_94:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_93
end_loop_93:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_95
start_loop_95:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_96
start_loop_96:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_97
start_loop_97:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_97
end_loop_97:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_96
end_loop_96:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_95
end_loop_95:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_98
start_loop_98:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_98
end_loop_98:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_99
start_loop_99:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_99
end_loop_99:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_92
end_loop_92:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_100
start_loop_100:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_101
start_loop_101:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_102
start_loop_102:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_102
end_loop_102:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_101
end_loop_101:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_103
start_loop_103:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_104
start_loop_104:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_105
start_loop_105:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_105
end_loop_105:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_104
end_loop_104:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_103
end_loop_103:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_100
end_loop_100:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_106
start_loop_106:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_106
end_loop_106:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_107
start_loop_107:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_108
start_loop_108:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_109
start_loop_109:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_109
end_loop_109:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_108
end_loop_108:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_110
start_loop_110:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_111
start_loop_111:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_112
start_loop_112:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_112
end_loop_112:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_111
end_loop_111:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_110
end_loop_110:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_113
start_loop_113:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_113
end_loop_113:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_114
start_loop_114:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_114
end_loop_114:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_107
end_loop_107:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_115
start_loop_115:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_116
start_loop_116:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_117
start_loop_117:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_117
end_loop_117:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_116
end_loop_116:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_118
start_loop_118:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_119
start_loop_119:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_120
start_loop_120:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_120
end_loop_120:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_119
end_loop_119:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_118
end_loop_118:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_115
end_loop_115:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_121
start_loop_121:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_121
end_loop_121:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_122
start_loop_122:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_123
start_loop_123:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_124
start_loop_124:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_124
end_loop_124:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_123
end_loop_123:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_125
start_loop_125:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_126
start_loop_126:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_127
start_loop_127:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_127
end_loop_127:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_126
end_loop_126:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_125
end_loop_125:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_128
start_loop_128:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_128
end_loop_128:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_129
start_loop_129:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_129
end_loop_129:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_122
end_loop_122:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_130
start_loop_130:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_130
end_loop_130:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_131
start_loop_131:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_132
start_loop_132:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_132
end_loop_132:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_133
start_loop_133:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_133
end_loop_133:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_131
end_loop_131:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_134
start_loop_134:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_135
start_loop_135:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_135
end_loop_135:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_134
end_loop_134:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_136
start_loop_136:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_137
start_loop_137:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_138
start_loop_138:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_138
end_loop_138:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_137
end_loop_137:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_136
end_loop_136:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_139
start_loop_139:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_139
end_loop_139:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_140
start_loop_140:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_140
end_loop_140:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_141
start_loop_141:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_141
end_loop_141:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_142
start_loop_142:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_142
end_loop_142:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_143
start_loop_143:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_143
end_loop_143:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_144
start_loop_144:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_144
end_loop_144:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_145
start_loop_145:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_145
end_loop_145:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_146
start_loop_146:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_146
end_loop_146:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_147
start_loop_147:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_148
start_loop_148:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_148
end_loop_148:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_147
end_loop_147:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_149
start_loop_149:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_150
start_loop_150:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_151
start_loop_151:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_151
end_loop_151:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_150
end_loop_150:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_149
end_loop_149:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_152
start_loop_152:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_152
end_loop_152:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_153
start_loop_153:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_153
end_loop_153:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_154
start_loop_154:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_154
end_loop_154:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_155
start_loop_155:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_155
end_loop_155:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_9
end_loop_9:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_156
start_loop_156:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $48, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-48, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_157
start_loop_157:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_157
end_loop_157:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)


movq    -8(%rbp), %r8
movb    (%r8), %cl
cmpb    $0, %cl
je      end_seek_loop_0
addq $1, %r8
subq    $32, %r8
start_seek_loop_0:
addq    $32, %r8
vmovdqa .four_offset_mask(%rip), %ymm0
vpor    (%r8), %ymm0, %ymm0
vpxor   %xmm1, %xmm1, %xmm1
vpcmpeqb        %ymm1, %ymm0, %ymm0
vpmovmskb       %ymm0, %eax
testl   %eax, %eax
je      start_seek_loop_0
bsfl    %eax, %eax
addq %rax, %r8
movq    %r8, -8(%rbp)
end_seek_loop_0:


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_158
start_loop_158:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_158
end_loop_158:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_156
end_loop_156:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_159
start_loop_159:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_160
start_loop_160:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_160
end_loop_160:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_159
end_loop_159:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_161
start_loop_161:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_162
start_loop_162:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_163
start_loop_163:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_163
end_loop_163:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_162
end_loop_162:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_161
end_loop_161:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_164
start_loop_164:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_164
end_loop_164:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_165
start_loop_165:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_165
end_loop_165:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_7
end_loop_7:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_166
start_loop_166:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_167
start_loop_167:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_168
start_loop_168:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_168
end_loop_168:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_167
end_loop_167:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_169
start_loop_169:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_170
start_loop_170:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_171
start_loop_171:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_171
end_loop_171:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_170
end_loop_170:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_169
end_loop_169:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_172
start_loop_172:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_172
end_loop_172:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_173
start_loop_173:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_173
end_loop_173:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_174
start_loop_174:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_175
start_loop_175:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $10, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-10, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_176
start_loop_176:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_177
start_loop_177:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_177
end_loop_177:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_176
end_loop_176:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_178
start_loop_178:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_179
start_loop_179:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_180
start_loop_180:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_180
end_loop_180:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_179
end_loop_179:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_178
end_loop_178:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_181
start_loop_181:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_181
end_loop_181:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_182
start_loop_182:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_182
end_loop_182:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_183
start_loop_183:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_183
end_loop_183:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $10, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-10, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_184
start_loop_184:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_185
start_loop_185:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_185
end_loop_185:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_184
end_loop_184:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_186
start_loop_186:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_187
start_loop_187:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_188
start_loop_188:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_188
end_loop_188:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_187
end_loop_187:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_186
end_loop_186:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_189
start_loop_189:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_189
end_loop_189:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_190
start_loop_190:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_190
end_loop_190:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_191
start_loop_191:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_191
end_loop_191:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_192
start_loop_192:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $14, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-14, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $3, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $9, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-220, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $16, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-221, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-16, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_193
start_loop_193:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_193
end_loop_193:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)


movq    -8(%rbp), %r8
movb    (%r8), %cl
cmpb    $0, %cl
je      end_seek_loop_1
addq $1, %r8
subq    $32, %r8
start_seek_loop_1:
addq    $32, %r8
vmovdqa .four_offset_mask(%rip), %ymm0
vpor    (%r8), %ymm0, %ymm0
vpxor   %xmm1, %xmm1, %xmm1
vpcmpeqb        %ymm1, %ymm0, %ymm0
vpmovmskb       %ymm0, %eax
testl   %eax, %eax
je      start_seek_loop_1
bsfl    %eax, %eax
addq %rax, %r8
movq    %r8, -8(%rbp)
end_seek_loop_1:


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $12, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-180, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-12, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $16, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-181, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-16, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_194
start_loop_194:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_194
end_loop_194:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)


movq    -8(%rbp), %r8
movb    (%r8), %cl
cmpb    $0, %cl
je      end_seek_loop_2
addq $1, %r8
subq    $32, %r8
start_seek_loop_2:
addq    $32, %r8
vmovdqa .four_offset_mask(%rip), %ymm0
vpor    (%r8), %ymm0, %ymm0
vpxor   %xmm1, %xmm1, %xmm1
vpcmpeqb        %ymm1, %ymm0, %ymm0
vpmovmskb       %ymm0, %eax
testl   %eax, %eax
je      start_seek_loop_2
bsfl    %eax, %eax
addq %rax, %r8
movq    %r8, -8(%rbp)
end_seek_loop_2:


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-140, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $16, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-141, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-16, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_195
start_loop_195:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_195
end_loop_195:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)


movq    -8(%rbp), %r8
movb    (%r8), %cl
cmpb    $0, %cl
je      end_seek_loop_3
addq $1, %r8
subq    $32, %r8
start_seek_loop_3:
addq    $32, %r8
vmovdqa .four_offset_mask(%rip), %ymm0
vpor    (%r8), %ymm0, %ymm0
vpxor   %xmm1, %xmm1, %xmm1
vpcmpeqb        %ymm1, %ymm0, %ymm0
vpmovmskb       %ymm0, %eax
testl   %eax, %eax
je      start_seek_loop_3
bsfl    %eax, %eax
addq %rax, %r8
movq    %r8, -8(%rbp)
end_seek_loop_3:


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_192
end_loop_192:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_196
start_loop_196:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $12, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-12, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-12, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_196
end_loop_196:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_197
start_loop_197:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $12, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-12, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-12, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_197
end_loop_197:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_175
end_loop_175:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_198
start_loop_198:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_199
start_loop_199:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_199
end_loop_199:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_198
end_loop_198:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_200
start_loop_200:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_201
start_loop_201:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_202
start_loop_202:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_202
end_loop_202:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_201
end_loop_201:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_200
end_loop_200:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_203
start_loop_203:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_203
end_loop_203:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_204
start_loop_204:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_204
end_loop_204:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_205
start_loop_205:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_206
start_loop_206:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_207
start_loop_207:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_207
end_loop_207:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_206
end_loop_206:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_208
start_loop_208:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_209
start_loop_209:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_210
start_loop_210:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_210
end_loop_210:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_209
end_loop_209:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_208
end_loop_208:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_211
start_loop_211:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_211
end_loop_211:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_212
start_loop_212:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_212
end_loop_212:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_213
start_loop_213:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $56, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-56, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_213
end_loop_213:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_214
start_loop_214:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_215
start_loop_215:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_215
end_loop_215:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_214
end_loop_214:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_216
start_loop_216:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_217
start_loop_217:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_218
start_loop_218:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_218
end_loop_218:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_217
end_loop_217:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_216
end_loop_216:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_219
start_loop_219:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_219
end_loop_219:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_220
start_loop_220:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_220
end_loop_220:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_221
start_loop_221:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $138, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-138, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_221
end_loop_221:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_222
start_loop_222:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_223
start_loop_223:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_223
end_loop_223:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_222
end_loop_222:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_224
start_loop_224:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_225
start_loop_225:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_226
start_loop_226:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_226
end_loop_226:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_225
end_loop_225:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_224
end_loop_224:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_227
start_loop_227:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_227
end_loop_227:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_228
start_loop_228:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_228
end_loop_228:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_229
start_loop_229:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $97, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-97, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_229
end_loop_229:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_230
start_loop_230:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_231
start_loop_231:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_231
end_loop_231:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_230
end_loop_230:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_232
start_loop_232:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_233
start_loop_233:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_234
start_loop_234:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_234
end_loop_234:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_233
end_loop_233:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_232
end_loop_232:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_235
start_loop_235:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_235
end_loop_235:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_236
start_loop_236:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_236
end_loop_236:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_237
start_loop_237:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $56, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-56, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-17, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_238
start_loop_238:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_238
end_loop_238:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_239
start_loop_239:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_239
end_loop_239:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_237
end_loop_237:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_240
start_loop_240:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_241
start_loop_241:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_241
end_loop_241:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_240
end_loop_240:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_242
start_loop_242:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_243
start_loop_243:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_244
start_loop_244:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_244
end_loop_244:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_243
end_loop_243:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_242
end_loop_242:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_245
start_loop_245:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_245
end_loop_245:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_246
start_loop_246:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_246
end_loop_246:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_247
start_loop_247:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $138, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-138, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-99, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_248
start_loop_248:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_248
end_loop_248:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_249
start_loop_249:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_249
end_loop_249:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $93, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_247
end_loop_247:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-15, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_250
start_loop_250:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_251
start_loop_251:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_251
end_loop_251:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_250
end_loop_250:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_252
start_loop_252:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_253
start_loop_253:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_254
start_loop_254:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_254
end_loop_254:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_253
end_loop_253:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_252
end_loop_252:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_255
start_loop_255:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_255
end_loop_255:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_256
start_loop_256:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_256
end_loop_256:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_257
start_loop_257:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $97, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-97, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-58, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_258
start_loop_258:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_258
end_loop_258:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_259
start_loop_259:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_259
end_loop_259:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $52, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_257
end_loop_257:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_260
start_loop_260:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_261
start_loop_261:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_261
end_loop_261:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_260
end_loop_260:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_262
start_loop_262:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_263
start_loop_263:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_264
start_loop_264:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_264
end_loop_264:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_263
end_loop_263:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_262
end_loop_262:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_265
start_loop_265:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_265
end_loop_265:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_266
start_loop_266:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_266
end_loop_266:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_267
start_loop_267:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-16, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $56, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-56, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-17, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_268
start_loop_268:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_268
end_loop_268:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)


movq    -8(%rbp), %r8
movb    (%r8), %cl
cmpb    $0, %cl
je      end_seek_loop_4
addq $1, %r8
subq    $32, %r8
start_seek_loop_4:
addq    $32, %r8
vmovdqa .four_offset_mask(%rip), %ymm0
vpor    (%r8), %ymm0, %ymm0
vpxor   %xmm1, %xmm1, %xmm1
vpcmpeqb        %ymm1, %ymm0, %ymm0
vpmovmskb       %ymm0, %eax
testl   %eax, %eax
je      start_seek_loop_4
bsfl    %eax, %eax
addq %rax, %r8
movq    %r8, -8(%rbp)
end_seek_loop_4:


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_267
end_loop_267:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_269
start_loop_269:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_270
start_loop_270:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_270
end_loop_270:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_269
end_loop_269:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_271
start_loop_271:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_272
start_loop_272:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_273
start_loop_273:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_273
end_loop_273:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_272
end_loop_272:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_271
end_loop_271:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_274
start_loop_274:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_274
end_loop_274:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_275
start_loop_275:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_275
end_loop_275:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_276
start_loop_276:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-98, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $138, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-138, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-99, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_277
start_loop_277:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_277
end_loop_277:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)


movq    -8(%rbp), %r8
movb    (%r8), %cl
cmpb    $0, %cl
je      end_seek_loop_5
addq $1, %r8
subq    $32, %r8
start_seek_loop_5:
addq    $32, %r8
vmovdqa .four_offset_mask(%rip), %ymm0
vpor    (%r8), %ymm0, %ymm0
vpxor   %xmm1, %xmm1, %xmm1
vpcmpeqb        %ymm1, %ymm0, %ymm0
vpmovmskb       %ymm0, %eax
testl   %eax, %eax
je      start_seek_loop_5
bsfl    %eax, %eax
addq %rax, %r8
movq    %r8, -8(%rbp)
end_seek_loop_5:


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_276
end_loop_276:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_278
start_loop_278:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_279
start_loop_279:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_279
end_loop_279:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_278
end_loop_278:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_280
start_loop_280:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_281
start_loop_281:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_282
start_loop_282:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_282
end_loop_282:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_281
end_loop_281:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_280
end_loop_280:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_283
start_loop_283:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_283
end_loop_283:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_284
start_loop_284:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_284
end_loop_284:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_285
start_loop_285:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-57, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $97, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-97, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-58, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_286
start_loop_286:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_286
end_loop_286:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)


movq    -8(%rbp), %r8
movb    (%r8), %cl
cmpb    $0, %cl
je      end_seek_loop_6
addq $1, %r8
subq    $32, %r8
start_seek_loop_6:
addq    $32, %r8
vmovdqa .four_offset_mask(%rip), %ymm0
vpor    (%r8), %ymm0, %ymm0
vpxor   %xmm1, %xmm1, %xmm1
vpcmpeqb        %ymm1, %ymm0, %ymm0
vpmovmskb       %ymm0, %eax
testl   %eax, %eax
je      start_seek_loop_6
bsfl    %eax, %eax
addq %rax, %r8
movq    %r8, -8(%rbp)
end_seek_loop_6:


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_285
end_loop_285:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_287
start_loop_287:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_288
start_loop_288:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_288
end_loop_288:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_287
end_loop_287:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_289
start_loop_289:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_290
start_loop_290:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_291
start_loop_291:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_291
end_loop_291:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_290
end_loop_290:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_289
end_loop_289:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_292
start_loop_292:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_292
end_loop_292:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_293
start_loop_293:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_293
end_loop_293:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_294
start_loop_294:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $56, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-56, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_294
end_loop_294:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_295
start_loop_295:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_296
start_loop_296:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_296
end_loop_296:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_295
end_loop_295:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_297
start_loop_297:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_298
start_loop_298:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_299
start_loop_299:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_299
end_loop_299:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_298
end_loop_298:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_297
end_loop_297:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_300
start_loop_300:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_300
end_loop_300:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_301
start_loop_301:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_301
end_loop_301:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_302
start_loop_302:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $138, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-138, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_302
end_loop_302:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-13, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_303
start_loop_303:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_304
start_loop_304:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_304
end_loop_304:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_303
end_loop_303:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_305
start_loop_305:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_306
start_loop_306:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_307
start_loop_307:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_307
end_loop_307:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_306
end_loop_306:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_305
end_loop_305:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_308
start_loop_308:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_308
end_loop_308:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_309
start_loop_309:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_309
end_loop_309:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_310
start_loop_310:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $97, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-97, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_310
end_loop_310:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_311
start_loop_311:
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_312
start_loop_312:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $18, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-18, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_312
end_loop_312:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_313
start_loop_313:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $16, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-16, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_313
end_loop_313:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_314
start_loop_314:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_315
start_loop_315:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_315
end_loop_315:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_314
end_loop_314:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_316
start_loop_316:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_317
start_loop_317:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_318
start_loop_318:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_318
end_loop_318:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_317
end_loop_317:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_316
end_loop_316:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_319
start_loop_319:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_319
end_loop_319:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_320
start_loop_320:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_320
end_loop_320:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_321
start_loop_321:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_321
end_loop_321:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_322
start_loop_322:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_323
start_loop_323:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_323
end_loop_323:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_322
end_loop_322:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_324
start_loop_324:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_325
start_loop_325:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_326
start_loop_326:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_326
end_loop_326:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_325
end_loop_325:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_324
end_loop_324:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_327
start_loop_327:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_327
end_loop_327:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_328
start_loop_328:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_328
end_loop_328:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_329
start_loop_329:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_329
end_loop_329:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-8, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_330
start_loop_330:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_330
end_loop_330:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-4, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-4, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_331
start_loop_331:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_332
start_loop_332:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_332
end_loop_332:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_331
end_loop_331:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_333
start_loop_333:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_334
start_loop_334:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_335
start_loop_335:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_335
end_loop_335:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_334
end_loop_334:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_333
end_loop_333:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_336
start_loop_336:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_336
end_loop_336:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_337
start_loop_337:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_337
end_loop_337:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_338
start_loop_338:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_338
end_loop_338:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_339
start_loop_339:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_340
start_loop_340:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_341
start_loop_341:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_341
end_loop_341:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_340
end_loop_340:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_342
start_loop_342:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_343
start_loop_343:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_344
start_loop_344:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_344
end_loop_344:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_343
end_loop_343:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_342
end_loop_342:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_339
end_loop_339:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_345
start_loop_345:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_345
end_loop_345:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_346
start_loop_346:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_347
start_loop_347:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_348
start_loop_348:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_348
end_loop_348:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_347
end_loop_347:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_349
start_loop_349:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_350
start_loop_350:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_351
start_loop_351:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_351
end_loop_351:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_350
end_loop_350:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_349
end_loop_349:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_352
start_loop_352:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_352
end_loop_352:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_353
start_loop_353:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_353
end_loop_353:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_346
end_loop_346:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_354
start_loop_354:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_355
start_loop_355:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_356
start_loop_356:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_356
end_loop_356:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_355
end_loop_355:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_357
start_loop_357:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_358
start_loop_358:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_359
start_loop_359:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_359
end_loop_359:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_358
end_loop_358:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_357
end_loop_357:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_354
end_loop_354:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_360
start_loop_360:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_360
end_loop_360:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_361
start_loop_361:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_362
start_loop_362:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_363
start_loop_363:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_363
end_loop_363:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_362
end_loop_362:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_364
start_loop_364:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_365
start_loop_365:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_366
start_loop_366:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_366
end_loop_366:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_365
end_loop_365:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_364
end_loop_364:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_367
start_loop_367:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_367
end_loop_367:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_368
start_loop_368:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_368
end_loop_368:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_361
end_loop_361:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_369
start_loop_369:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_370
start_loop_370:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_371
start_loop_371:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_371
end_loop_371:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_370
end_loop_370:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_372
start_loop_372:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_373
start_loop_373:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_374
start_loop_374:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_374
end_loop_374:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_373
end_loop_373:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_372
end_loop_372:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_369
end_loop_369:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_375
start_loop_375:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_375
end_loop_375:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_376
start_loop_376:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_377
start_loop_377:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_378
start_loop_378:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_378
end_loop_378:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_377
end_loop_377:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_379
start_loop_379:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_380
start_loop_380:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_381
start_loop_381:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_381
end_loop_381:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_380
end_loop_380:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_379
end_loop_379:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_382
start_loop_382:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_382
end_loop_382:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_383
start_loop_383:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_383
end_loop_383:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_376
end_loop_376:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_384
start_loop_384:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_384
end_loop_384:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_385
start_loop_385:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_386
start_loop_386:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_386
end_loop_386:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_387
start_loop_387:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_387
end_loop_387:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_385
end_loop_385:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_388
start_loop_388:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_389
start_loop_389:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_390
start_loop_390:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_390
end_loop_390:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_389
end_loop_389:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_391
start_loop_391:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_392
start_loop_392:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_393
start_loop_393:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_393
end_loop_393:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_392
end_loop_392:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_391
end_loop_391:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_388
end_loop_388:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_394
start_loop_394:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_394
end_loop_394:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_395
start_loop_395:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_396
start_loop_396:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_397
start_loop_397:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_397
end_loop_397:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_396
end_loop_396:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_398
start_loop_398:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_399
start_loop_399:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_400
start_loop_400:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_400
end_loop_400:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_399
end_loop_399:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_398
end_loop_398:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_401
start_loop_401:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_401
end_loop_401:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_402
start_loop_402:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_402
end_loop_402:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_395
end_loop_395:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_403
start_loop_403:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_404
start_loop_404:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_405
start_loop_405:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_405
end_loop_405:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_404
end_loop_404:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_406
start_loop_406:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_407
start_loop_407:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_408
start_loop_408:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_408
end_loop_408:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_407
end_loop_407:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_406
end_loop_406:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_403
end_loop_403:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_409
start_loop_409:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_409
end_loop_409:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_410
start_loop_410:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_411
start_loop_411:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_412
start_loop_412:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_412
end_loop_412:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_411
end_loop_411:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_413
start_loop_413:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_414
start_loop_414:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_415
start_loop_415:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_415
end_loop_415:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_414
end_loop_414:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_413
end_loop_413:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_416
start_loop_416:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_416
end_loop_416:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_417
start_loop_417:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_417
end_loop_417:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_410
end_loop_410:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_418
start_loop_418:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_419
start_loop_419:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_420
start_loop_420:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_420
end_loop_420:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_419
end_loop_419:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_421
start_loop_421:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_422
start_loop_422:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_423
start_loop_423:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_423
end_loop_423:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_422
end_loop_422:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_421
end_loop_421:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_418
end_loop_418:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_424
start_loop_424:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_424
end_loop_424:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_425
start_loop_425:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_426
start_loop_426:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_427
start_loop_427:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_427
end_loop_427:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_426
end_loop_426:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_428
start_loop_428:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_429
start_loop_429:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_430
start_loop_430:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_430
end_loop_430:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_429
end_loop_429:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_428
end_loop_428:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_431
start_loop_431:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_431
end_loop_431:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_432
start_loop_432:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_432
end_loop_432:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_425
end_loop_425:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_433
start_loop_433:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_433
end_loop_433:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_434
start_loop_434:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_435
start_loop_435:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_435
end_loop_435:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_436
start_loop_436:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_436
end_loop_436:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_434
end_loop_434:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_437
start_loop_437:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_438
start_loop_438:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_438
end_loop_438:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_437
end_loop_437:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_439
start_loop_439:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_440
start_loop_440:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_441
start_loop_441:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_441
end_loop_441:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_440
end_loop_440:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_439
end_loop_439:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_442
start_loop_442:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_442
end_loop_442:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_443
start_loop_443:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_443
end_loop_443:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_444
start_loop_444:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_444
end_loop_444:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_445
start_loop_445:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_445
end_loop_445:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_446
start_loop_446:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_446
end_loop_446:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_447
start_loop_447:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_447
end_loop_447:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_448
start_loop_448:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_448
end_loop_448:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_449
start_loop_449:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_449
end_loop_449:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_450
start_loop_450:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_451
start_loop_451:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_451
end_loop_451:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_450
end_loop_450:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_452
start_loop_452:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_453
start_loop_453:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_454
start_loop_454:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_454
end_loop_454:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_453
end_loop_453:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_452
end_loop_452:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_455
start_loop_455:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_455
end_loop_455:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_456
start_loop_456:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_456
end_loop_456:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_457
start_loop_457:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_457
end_loop_457:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_458
start_loop_458:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_458
end_loop_458:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_311
end_loop_311:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_459
start_loop_459:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_460
start_loop_460:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_461
start_loop_461:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_461
end_loop_461:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_460
end_loop_460:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_462
start_loop_462:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_463
start_loop_463:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_464
start_loop_464:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_464
end_loop_464:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_463
end_loop_463:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_462
end_loop_462:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_459
end_loop_459:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_465
start_loop_465:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_465
end_loop_465:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_466
start_loop_466:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_467
start_loop_467:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_468
start_loop_468:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_468
end_loop_468:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_467
end_loop_467:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_469
start_loop_469:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_470
start_loop_470:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_471
start_loop_471:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_471
end_loop_471:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_470
end_loop_470:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_469
end_loop_469:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_472
start_loop_472:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_472
end_loop_472:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_473
start_loop_473:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_473
end_loop_473:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_466
end_loop_466:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_474
start_loop_474:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_475
start_loop_475:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_476
start_loop_476:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_476
end_loop_476:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_475
end_loop_475:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_477
start_loop_477:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_478
start_loop_478:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_479
start_loop_479:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_479
end_loop_479:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_478
end_loop_478:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_477
end_loop_477:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_474
end_loop_474:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_480
start_loop_480:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_480
end_loop_480:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_481
start_loop_481:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_482
start_loop_482:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_483
start_loop_483:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_483
end_loop_483:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_482
end_loop_482:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_484
start_loop_484:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_485
start_loop_485:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_486
start_loop_486:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_486
end_loop_486:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_485
end_loop_485:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_484
end_loop_484:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_487
start_loop_487:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_487
end_loop_487:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_488
start_loop_488:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_488
end_loop_488:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_481
end_loop_481:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_489
start_loop_489:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_490
start_loop_490:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_491
start_loop_491:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_491
end_loop_491:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_490
end_loop_490:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_492
start_loop_492:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_493
start_loop_493:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_494
start_loop_494:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_494
end_loop_494:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_493
end_loop_493:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_492
end_loop_492:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_489
end_loop_489:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_495
start_loop_495:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_495
end_loop_495:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_496
start_loop_496:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_497
start_loop_497:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_498
start_loop_498:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_498
end_loop_498:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_497
end_loop_497:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_499
start_loop_499:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_500
start_loop_500:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_501
start_loop_501:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_501
end_loop_501:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_500
end_loop_500:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_499
end_loop_499:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_502
start_loop_502:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_502
end_loop_502:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_503
start_loop_503:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_503
end_loop_503:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_496
end_loop_496:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_504
start_loop_504:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_504
end_loop_504:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_505
start_loop_505:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_506
start_loop_506:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_506
end_loop_506:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_507
start_loop_507:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_507
end_loop_507:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_505
end_loop_505:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_508
start_loop_508:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_509
start_loop_509:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_510
start_loop_510:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_510
end_loop_510:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_509
end_loop_509:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_511
start_loop_511:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_512
start_loop_512:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_513
start_loop_513:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_513
end_loop_513:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_512
end_loop_512:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_511
end_loop_511:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_508
end_loop_508:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_514
start_loop_514:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-7, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_514
end_loop_514:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_515
start_loop_515:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_516
start_loop_516:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_517
start_loop_517:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_517
end_loop_517:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_516
end_loop_516:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_518
start_loop_518:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_519
start_loop_519:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_520
start_loop_520:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_520
end_loop_520:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_519
end_loop_519:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_518
end_loop_518:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_521
start_loop_521:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_521
end_loop_521:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_522
start_loop_522:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_522
end_loop_522:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_515
end_loop_515:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_523
start_loop_523:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_524
start_loop_524:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_525
start_loop_525:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_525
end_loop_525:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_524
end_loop_524:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_526
start_loop_526:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_527
start_loop_527:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_528
start_loop_528:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_528
end_loop_528:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_527
end_loop_527:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_526
end_loop_526:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_523
end_loop_523:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_529
start_loop_529:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-6, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_529
end_loop_529:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_530
start_loop_530:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_531
start_loop_531:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_532
start_loop_532:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_532
end_loop_532:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_531
end_loop_531:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_533
start_loop_533:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_534
start_loop_534:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_535
start_loop_535:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_535
end_loop_535:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_534
end_loop_534:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_533
end_loop_533:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_536
start_loop_536:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_536
end_loop_536:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_537
start_loop_537:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_537
end_loop_537:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_530
end_loop_530:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_538
start_loop_538:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_539
start_loop_539:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_540
start_loop_540:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_540
end_loop_540:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_539
end_loop_539:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_541
start_loop_541:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_542
start_loop_542:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_543
start_loop_543:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_543
end_loop_543:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_542
end_loop_542:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_541
end_loop_541:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_538
end_loop_538:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_544
start_loop_544:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $-1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_544
end_loop_544:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_545
start_loop_545:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_546
start_loop_546:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_547
start_loop_547:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_547
end_loop_547:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_546
end_loop_546:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_548
start_loop_548:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_549
start_loop_549:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_550
start_loop_550:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_550
end_loop_550:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_549
end_loop_549:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_548
end_loop_548:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_551
start_loop_551:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_551
end_loop_551:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_552
start_loop_552:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_552
end_loop_552:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_545
end_loop_545:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_553
start_loop_553:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_553
end_loop_553:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_554
start_loop_554:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_555
start_loop_555:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_555
end_loop_555:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_556
start_loop_556:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_556
end_loop_556:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_554
end_loop_554:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %al
movq    stdout(%rip), %rsi
movsbl  %al, %edi
call    putc@PLT
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_557
start_loop_557:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_558
start_loop_558:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_558
end_loop_558:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_557
end_loop_557:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_205
end_loop_205:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-5, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_559
start_loop_559:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_560
start_loop_560:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_560
end_loop_560:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_559
end_loop_559:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_561
start_loop_561:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_562
start_loop_562:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_563
start_loop_563:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_563
end_loop_563:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_562
end_loop_562:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_561
end_loop_561:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_564
start_loop_564:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_564
end_loop_564:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_565
start_loop_565:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_565
end_loop_565:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_174
end_loop_174:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_566
start_loop_566:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-214, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_567
start_loop_567:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_567
end_loop_567:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_568
start_loop_568:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_568
end_loop_568:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $205, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-174, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_569
start_loop_569:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_569
end_loop_569:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_570
start_loop_570:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_570
end_loop_570:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $168, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-134, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-9, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_571
start_loop_571:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_571
end_loop_571:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-2, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_572
start_loop_572:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $4, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_572
end_loop_572:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $129, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_573
start_loop_573:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_574
start_loop_574:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_574
end_loop_574:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_573
end_loop_573:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_575
start_loop_575:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_576
start_loop_576:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_577
start_loop_577:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_577
end_loop_577:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_576
end_loop_576:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_575
end_loop_575:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_578
start_loop_578:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_578
end_loop_578:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_579
start_loop_579:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_580
start_loop_580:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_581
start_loop_581:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_581
end_loop_581:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_580
end_loop_580:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_582
start_loop_582:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_583
start_loop_583:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_584
start_loop_584:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_584
end_loop_584:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_583
end_loop_583:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_582
end_loop_582:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_585
start_loop_585:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_585
end_loop_585:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_579
end_loop_579:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_586
start_loop_586:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_587
start_loop_587:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_587
end_loop_587:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_586
end_loop_586:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_588
start_loop_588:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_589
start_loop_589:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_590
start_loop_590:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_590
end_loop_590:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_589
end_loop_589:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_588
end_loop_588:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_591
start_loop_591:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_591
end_loop_591:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_592
start_loop_592:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-11, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_593
start_loop_593:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_594
start_loop_594:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_594
end_loop_594:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_593
end_loop_593:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_595
start_loop_595:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
subb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_596
start_loop_596:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_597
start_loop_597:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-3, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_597
end_loop_597:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_596
end_loop_596:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_595
end_loop_595:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
je      end_loop_598
start_loop_598:
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movq   %rax, %r12
addq   $-1, %r12
movq    $1, %r15
imul   %rcx, %r15
addb    %r15b , (%r12)
movb    $0, (%rax)


movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movq    (%rax), %rcx
movb    $0, (%rax)


movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_598
end_loop_598:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_592
end_loop_592:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
addb    $1, %cl
movb    %cl, (%rax)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_566
end_loop_566:
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $-1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
movb    (%rax), %cl
cmpb    $0, %cl
jne      start_loop_166
end_loop_166:
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movq    -8(%rbp), %rax
addq    $1, %rax
movq    %rax, -8(%rbp)
movl    $0, %eax
movq    %rbp, %rsp
popq    %rbp
ret

